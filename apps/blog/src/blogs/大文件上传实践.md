---
title: 前端实现大文件上传
date: 2024-05-19 14:00:00
tags: [前端, 大文件上传]
---
记录一下如何对大文件进行上传，内容包括文件切片、断点续传、暂停上传、进度展示等功能
我这里使用React+Node来模拟实现
<!-- more -->
### 客户端
#### 文件切片
文件切片主要是用slice方法实现
```
//控制切片大小
const chunkSize = 10 * 1024; //10M
//文件切片汇总
function createFileChunk(file, size = chunkSize) {
    const fileChunkList = [];
    let curSize = 0
    //当curCount小于文件的尺寸时，继续切片
    while (curSize < file.size) {
        //将文件切片
        fileChunkList.push({
            file: file.slice(curSize, curSize + size)  //file的slice允许将一个文件按照大小切成多个片
        })

        curSize += size
    }

    return fileChunkList
}
```
#### 获取文件Hash值
为什么要获取Hash值？   
因为hash是根据文件内容来生成的，就算你改了文件名称，hash也是一样，不会存在重复上传的问题，计算hash用spark-md5来获取。  
文件太大，计算Hash是个非常耗时的任务，为了避免页面卡顿阻塞，我们可以用web-worker开启多线程去计算
hash及spark-md5文件放在工程的public文件下，具体代码如下
```
/* eslint-disable no-restricted-globals */
/**
 * 在 worker 中也是不允许访问 dom 的；
 * 但它提供了importScripts函数用于导入外部脚本，通过它导入spark-md5；
 * Worker中没有window，Worker中self指向顶层对象。
 */
// eslint-disable-next-line no-restricted-globals
self.importScripts('./spark-md5.min.js')
//self是web-worker的顶层对象
self.onmessage = e => {
    const { fileChunkList } = e.data
    const spark = new self.SparkMD5.ArrayBuffer()
    let percentage = 0
    let count = 0
    const loadNext = index => {
        const reader = new FileReader()
        reader.readAsArrayBuffer(fileChunkList[index].file)
        reader.onload = e => {
            count++
            spark.append(e.target.result)
            if (count === fileChunkList.length) {
                self.postMessage({
                    percentage: 100,
                    hash: spark.end()
                })
                self.close()
            } else {
                percentage += 100 / fileChunkList.length
                self.postMessage({
                    percentage
                })
                loadNext(count)
            }
        }
    }
    loadNext(count)
}
```
```
主线程获取hash，使用promise来获取
 const getFileHash = (fileChunkList) => {
        return new Promise((rel, rej) => {
            let woker = new Worker('/hash.js')
            woker.postMessage({ fileChunkList })
            woker.onmessage = (e) => {
                const { hash } = e.data
                if (hash) {
                    rel(hash)
                }
            }
        })
    }

//获取
const fileHash = await getFileHash(fileChunkList)
```
#### 文件上传
1. 文件上传前，需要先验证服务端是否存在已经上传的切片或者已经上传的整个文件，如果存在，则只上传未上传的部分。实现文件秒传功能
```
 // 验证该文件是否已上传
const data = await largeFileVerify(file.name, fileHash)
// 如果该文件已经上传
if (!data.shouldUpload) {
    message.success('该文件已上传')
    return
}
const requestList = []; //要上传的切片列表
fileChunkList.forEach((item, idx) => {
    const params = new FormData()
    //二进制的片文件
    params.append('chunk', item.file)
    //hash码，标识每一个文件
    params.append('hash', fileHash + '-hashIdx=' + idx)
    //上传的文件的名称
    params.append('filename', file.name)
    //上传文件的hash值
    params.append('fileHash', fileHash)
    //文件一共有多少片
    params.append('chunkNumber', fileChunkList.length)


    //过滤掉已经上传的切片
    if (!data.uploadedList.includes(params.get('hash'))) {
        requestList.push(params)
    }

    //开始上传
     Promise.all(requestList.map(item => largeFileUpload(item))).then(data => {
            //所有切片上传完成后，进行切片合并
            fileMerage(file.name,fileHash)
        })
        return
})

```
#### 进度展示
进度计算我是用(已经上传成功的切片/总的切片数量)*100来计算，具体代码如下
```
//上传成功的切片数量
const uploadedCount = useRef(0)
//上传的进度
const [progress, setprogress] = useState(0)

// 如果该文件已经上传,直接将进度拉到100
if (!data.shouldUpload) {
    message.success('该文件已上传')
    uploadedCount.current = 100
    setprogress(100)
    return
}

//如果存在部分已上传的切片
//已经上传的数量
uploadedCount.current = data.uploadedList.length  //uploadedList就是后端返回的已上传切片列表
let totProgress = ((uploadedCount.current / fileChunkList.length) * 100).toFixed(2)
setprogress(totProgress)


//文件上传函数中
 Promise.all(requestList.map(item => largeFileUpload(item).then(res => {
        //文件上传一个就自增1
        uploadedCount.current += 1
        //这里进度大于99的话，先展示99，等文件合并完成，再展示100
        let totProgress = ((uploadedCount.current / fileChunkList.length) * 100).toFixed(2)
        if (totProgress >= 99) {
            totProgress = 99.9
        }
        setprogress(totProgress)
        return res
    }))).then(data => {
        fileMerage(file.name,fileHash)
    })
    return

//文件合并函数
const fileMerage = async (filename,fileHash) => {
    const { code } = await largeFileMerage(filename,fileHash)
    if (code === 0) {
        //当合并成功
        setprogress(100)
        message.success('文件上传成功')
    }
}

<!-- 模板代码 -->
<Progress strokeColor={{ '0%': '#108ee9', '100%': '#87d068', }} percent={progress} size="small" />
```
#### 上传暂停
上传暂停功能我是用的axios的取消上传功能（[AbortController](https://www.axios-http.cn/docs/cancellation)）搭配redux来实现。
1. redux相关代码片段
```
import { createSlice } from '@reduxjs/toolkit'

const initialState = {
    //所有请求的controller
    fileAbortArr: [],
}

export const uploadFileSlice = createSlice({
    name: 'uploadFile',
    initialState,
    //定义reducers，并生成关联的操作
    reducers: {
        // action 里面有 type 和 payload 两个属性，所有的传参都在payload里面
        pushArr: (state, { payload }) => {
            state.fileAbortArr.push({
                controller: payload.controller,
                type: payload.type
            })
        },
        //清空
        clearAbort: (state) => {
            state.fileAbortArr = []
        },
        //移除
        filterArr: (state, { payload }) => {
            state.fileAbortArr = state.fileAbortArr.filter(d => d.type !== payload.type)
        }
    }
})


//导出方法
export const { pushArr, clearAbort,filterArr } = uploadFileSlice.actions;

//默认导出
export default uploadFileSlice.reducer
```
2. 组件内
```
import { filterArr, clearAbort } from '../../redux/features/uploadFileSlice';
import { useSelector, useDispatch } from 'react-redux'
const dispatch = useDispatch()
const { fileAbortArr } = useSelector((store) => store.uploadFile)

// 当用户点击暂停按钮后
//暂停上传
const filePause = () => {
    //取消所有还未上传的请求
    fileAbortArr.forEach(d => {
        d.controller.abort()
    })
    //清空数组
    dispatch(clearAbort())
}

```
3. 请求接口中
```
import { pushArr } from "../redux/features/uploadFileSlice";
// 大文件上传接口中，向数组push请求
function largeFileUpload(params) {
    const controller = new AbortController();
    store.dispatch(pushArr({ controller, type: params.get('hash') }))
    return instance.post('/largeFile/upload', params, {
        headers: { 'Content-Type': 'multipart/form-data' },
        signal: controller.signal
    })
}

```
至此，整个文件上传流程就结束了!

### 服务端
服务端处理文件和切片使用multiparty模块
> npm i multiparty --save

文件和切片保存的路径如图
![文件保存路径](../images/1.png)
具体代码如下
#### 文件上传接口
```
var express = require('express');
const router = express.Router();


//处理大文件切片必要模块
const multiparty = require('multiparty')
const EventEmitter = require('events');
const fs = require('fs');
const path = require('path');

const STATIC_TEMPORARY = path.resolve(__dirname, '../../public/largeFile/chunkFile'); //文件切片保存位置
const STATIC_FILES = path.resolve(__dirname, '../../public/largeFile/mergeFile') //文件保存地址

// 提取文件后缀名
const extractExt = filename => {
    return filename.slice(filename.lastIndexOf('.'), filename.length)
}
//大文件切片上传接口
router.post('/largeFile/upload', (req, res) => {
    const form = new multiparty.Form()
    const myEmitter = new EventEmitter()
    const formData = {
        filename: undefined,
        hash: undefined,
        chunk: undefined,
        fileHash: undefined
    }

    let isFieldOk = false,
        isFileOk = false;

    form.parse(req, (err, fields, files) => {
        if (err) {
            console.error('文件上传失败：', err);
            res.status(500).json({ error: '文件上传失败' });
            return;
        }
        formData.filename = fields['filename'][0];
        formData.hash = fields['hash'][0];
        formData.fileHash = fields['fileHash'][0];

        isFieldOk = true;
        myEmitter.emit('start');
    })

    //当有文件上传时，该事件会被触发，并执行回调函数
    form.on('file', function(name, file) {
        formData.chunk = file;
        isFileOk = true;
        myEmitter.emit('start');
    });

    myEmitter.on('start', () => {
        if (isFieldOk && isFileOk) {
            console.log('开始切片', formData)
            const { filename, hash, chunk, fileHash } = formData
            const dir = `${STATIC_TEMPORARY}/${fileHash}${extractExt(filename)}`
            try {
                //检查目标目录是否存在，如果不存在则创建目录
                if (!fs.existsSync(dir)) fs.mkdirSync(dir);

                //读取切片文件的内容，将切片文件的内容写入到目标目录下的文件中
                const buffer = fs.readFileSync(chunk.path);
                const ws = fs.createWriteStream(`${dir}/${hash}`);
                ws.write(buffer);
                ws.close();
                res.send({
                    code: 0,
                    msg: `${hash} 切片上传成功`
                })
            } catch (error) {
                console.error(error);
            }
            isFieldOk = false;
            isFileOk = false;
        }

    })
})









module.exports = router;
```
#### 文件合并接口
```
const { Buffer } = require('buffer')
router.get('/largeFile/merage', (req, res) => {
    const { filename,fileHash } = req.query


    //文件hash名称
    const fileHashPath = `${STATIC_TEMPORARY}/${fileHash}${extractExt(filename)}`
    try {
        let len = 0;
        //先排序 不然会将索引10的分块排在第3位
        const sortList = fs.readdirSync(fileHashPath).sort((a, b) => {
            //根据hashIdx来排序
            return Number(a.split('-hashIdx=')[1]) - Number(b.split('-hashIdx=')[1])
        })
        const bufferList = sortList.map(hash => {
            const buffer = fs.readFileSync(`${fileHashPath}/${hash}`);
            len += buffer.length;
            return buffer;
        });
        const buffer = Buffer.concat(bufferList, len);
        const ws = fs.createWriteStream(`${STATIC_FILES}/${filename}`);
        ws.write(buffer);
        ws.close();


        //删除切片
        deleteFolder(fileHashPath)

        res.send({
            code: 0,
            msg: '切片合成完成'
        });

    } catch (e) {
        console.error(error);
    }
})

//合成后，删除切片文件
function deleteFolder(filepath) {
    if (fs.existsSync(filepath)) {
        fs.readdirSync(filepath).forEach(filename => {
            const fp = `${filepath}/${filename}`;
            if (fs.statSync(fp).isDirectory()) deleteFolder(fp);
            else fs.unlinkSync(fp);
        });
        fs.rmdirSync(filepath);
    }
}
```
#### 文件校验接口
```
//校验文件是否已经上传
router.post('/largeFile/veirfy', (req, res) => {
    const { filename,fileHash } = req.body
    const filePath = `${STATIC_FILES}/${filename}`
    console.log(filePath)
    //如果已经存在该文件，直接返回
    if (fs.existsSync(filePath)) {
        res.send({
            code: 0,
            shouldUpload: false
        })
    } else { //如果不存在则计算已经上传了多少切片
        res.send({
            code: 0,
            shouldUpload: true,
            //已上传的文件列表数组
            uploadedList: fs.existsSync(`${STATIC_TEMPORARY}/${fileHash}${extractExt(filename)}`) ? fs.readdirSync(`${STATIC_TEMPORARY}/${fileHash}${extractExt(filename)}`) : []
        })
    }
})

```
### 最后效果
![效果演示](../images/2.gif)
参考资源：
1. https://cloud.tencent.com/developer/article/1589057
2. https://blog.csdn.net/csdn_yudong/article/details/123720232


